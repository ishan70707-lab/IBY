{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13088486,"sourceType":"datasetVersion","datasetId":8290171}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q uninstall -y transformers trl peft accelerate > /dev/null\n!pip -q install \"transformers==4.45.2\" \"trl==0.9.6\" \"peft==0.12.0\" \"accelerate>=0.34.2\" bitsandbytes datasets\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, json, random, gc, math\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\n# Base model — 3B fits T4 comfortably; upgrade to 7B if you have VRAM headroom\nBASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n\n# Kaggle dataset path\nDATA_PATH = \"/kaggle/input/essaydata/train.csv\"\n\ndef set_pad_token(tok):\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    return tok\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(DATA_PATH)\nneeded = [\"full_text\",\"cohesion\",\"syntax\",\"vocabulary\",\"phraseology\",\"grammar\",\"conventions\"]\ndf = df[needed].dropna().reset_index(drop=True)\n\ndef build_pair(r):\n    instr = (\n        \"Grade this student essay on six dimensions (1.0–5.0, halves allowed): \"\n        \"cohesion, syntax, vocabulary, phraseology, grammar, conventions. \"\n        \"Return STRICT JSON with exactly these keys and numeric values.\"\n        f\"\\n\\nEssay:\\n{r['full_text']}\"\n    )\n    out = {\n        \"cohesion\": float(r[\"cohesion\"]),\n        \"syntax\": float(r[\"syntax\"]),\n        \"vocabulary\": float(r[\"vocabulary\"]),\n        \"phraseology\": float(r[\"phraseology\"]),\n        \"grammar\": float(r[\"grammar\"]),\n        \"conventions\": float(r[\"conventions\"]),\n    }\n    return {\"input\": instr, \"output\": json.dumps(out, ensure_ascii=False)}\n\npairs = [build_pair(r) for _, r in df.iterrows()]\nrandom.shuffle(pairs)\nsplit = int(0.9 * len(pairs))\nds = DatasetDict({\n    \"train\": Dataset.from_list(pairs[:split]),\n    \"test\":  Dataset.from_list(pairs[split:])\n})\n\ntok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\ntok = set_pad_token(tok)\n\ndef to_chat(example):\n    sys = \"You are a strict writing teacher. Always return strict JSON with the six scores.\"\n    text = (\n        f\"<|system|>\\n{sys}\\n\"\n        f\"<|user|>\\n{example['input']}\\n\"\n        f\"<|assistant|>\\n{example['output']}\"\n    )\n    return {\"text\": text}\n\nds = ds.map(to_chat, remove_columns=ds[\"train\"].column_names)\nlen(ds[\"train\"]), len(ds[\"test\"])\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache(); gc.collect()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_4bit=True,  # QLoRA path\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n)\n\npeft_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # Qwen attention projections\n)\n\nargs = TrainingArguments(\n    output_dir=\"./qwen-ell-lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,  # less accumulation = faster, but less stable\n    learning_rate=5e-4,\n    num_train_epochs=0.1,           # only 10% of an epoch\n    max_steps=100,                  # cap steps so it stops early\n    logging_steps=10,\n    save_steps=50,\n    bf16=torch.cuda.is_available(),\n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    max_grad_norm=0.3,\n)\n\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tok,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"test\"],\n    dataset_text_field=\"text\",\n    peft_config=peft_cfg,\n    max_seq_length=1024,   # if OOM: drop to 768\n    args=args,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reload base + latest LoRA checkpoint\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n)\n\n# pick latest checkpoint inside ./qwen-ell-lora\nckpts = sorted([p for p in os.listdir(\"./qwen-ell-lora\") if p.startswith(\"checkpoint-\")],\n               key=lambda x: int(x.split(\"-\")[-1]))\nadapter_path = os.path.join(\"./qwen-ell-lora\", ckpts[-1]) if ckpts else \"./qwen-ell-lora\"\nft = PeftModel.from_pretrained(base, adapter_path)\n\n@torch.inference_mode()\ndef predict_ell_scores(essay: str, max_new_tokens=220, temperature=0.2, top_p=0.9):\n    sys = \"You are a strict writing teacher. Always return strict JSON with the six scores.\"\n    user = (\"Grade this student essay on six dimensions (1.0–5.0, halves allowed): \"\n            \"cohesion, syntax, vocabulary, phraseology, grammar, conventions. \"\n            \"Return STRICT JSON with exactly these keys and numeric values.\"\n            f\"\\n\\nEssay:\\n{essay}\")\n    prompt = tok.apply_chat_template(\n        [{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n        tokenize=False, add_generation_prompt=True\n    )\n    ids = tok([prompt], return_tensors=\"pt\").to(ft.device)\n    out = ft.generate(**ids, max_new_tokens=max_new_tokens, do_sample=True,\n                      temperature=temperature, top_p=top_p, pad_token_id=tok.eos_token_id)\n    txt = tok.decode(out[0][ids[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n    if not m:\n        return {\"raw\": txt}\n    cleaned = re.sub(r\",\\s*([}\\]])\", r\"\\1\", m.group(0))  # remove trailing commas\n    return json.loads(cleaned)\n\n# smoke test\nsample_text = df.loc[0, \"full_text\"]\npred6 = predict_ell_scores(sample_text)\npred6\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_ell_to_agent(ell_json):\n    # expects: cohesion, syntax, vocabulary, phraseology, grammar, conventions (1–5)\n    c = float(ell_json[\"cohesion\"])\n    s = float(ell_json[\"syntax\"])\n    v = float(ell_json[\"vocabulary\"])\n    p = float(ell_json[\"phraseology\"])\n    g = float(ell_json[\"grammar\"])\n    conv = float(ell_json[\"conventions\"])\n\n    # convert to 0–1 proxies for your rubric\n    relevance = (c + v + p) / 3.0 / 5.0\n    grammar   = (g + 0.5*conv) / 1.5 / 5.0\n    structure = (c + s) / 2.0 / 5.0\n    depth     = (v + p + s) / 3.0 / 5.0\n\n    return {\n        \"relevance_score\": relevance,\n        \"grammar_score\":   grammar,\n        \"structure_score\": structure,\n        \"depth_score\":     depth,\n    }\n\ndef combine_final(scores, weights=(0.3,0.2,0.2,0.3)):\n    rel, gra, struct, dep = scores[\"relevance_score\"], scores[\"grammar_score\"], scores[\"structure_score\"], scores[\"depth_score\"]\n    return float(round(rel*weights[0] + gra*weights[1] + struct*weights[2] + dep*weights[3], 4))\n\nmapped = map_ell_to_agent(pred6)\nfinal_score = combine_final(mapped)\nmapped | {\"final_score\": final_score}\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.874Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def grade_essay(essay: str):\n    ell = predict_ell_scores(essay)\n    if \"cohesion\" not in ell:\n        return {\"error\": \"model did not return JSON\", \"raw\": ell}\n    scores4 = map_ell_to_agent(ell)\n    scores4[\"final_score\"] = combine_final(scores4)\n    return {\"ell_scores\": ell, **scores4}\n\n# try it\nstudent_essay = \"\"\"The Impact of Artificial Intelligence on Modern Society\n\n    Artificial Intelligence (AI) has become an integral part of our daily lives, \n    revolutionizing various sectors including healthcare, finance, and transportation. \n    This essay explores the profound effects of AI on modern society, discussing both \n    its benefits and potential challenges.\n\n    One of the most significant impacts of AI is in the healthcare industry. \n    AI-powered diagnostic tools can analyze medical images with high accuracy, \n    often surpassing human capabilities. This leads to earlier detection of diseases \n    and more effective treatment plans. Moreover, AI algorithms can process vast \n    amounts of medical data to identify patterns and insights that might escape \n    human observation, potentially leading to breakthroughs in drug discovery and \n    personalized medicine.\n\n    In the financial sector, AI has transformed the way transactions are processed \n    and monitored. Machine learning algorithms can detect fraudulent activities in \n    real-time, enhancing security for consumers and institutions alike. Robo-advisors \n    use AI to provide personalized investment advice, democratizing access to \n    financial planning services.\n\n    The transportation industry is another area where AI is making significant strides. \n    Self-driving cars, powered by complex AI systems, promise to reduce accidents \n    caused by human error and provide mobility solutions for those unable to drive. \n    In logistics, AI optimizes routing and inventory management, leading to more \n    efficient supply chains and reduced environmental impact.\n\n    However, the rapid advancement of AI also presents challenges. There are concerns \n    about job displacement as AI systems become capable of performing tasks \n    traditionally done by humans. This raises questions about the need for retraining \n    and reskilling the workforce to adapt to an AI-driven economy.\n\n    Privacy and ethical concerns also arise with the increasing use of AI. The vast \n    amount of data required to train AI systems raises questions about data privacy \n    and consent. Additionally, there are ongoing debates about the potential biases \n    in AI algorithms and the need for transparent and accountable AI systems.\n\n    In conclusion, while AI offers tremendous benefits and has the potential to solve \n    some of humanity's most pressing challenges, it also requires careful consideration \n    of its societal implications. As we continue to integrate AI into various aspects \n    of our lives, it is crucial to strike a balance between technological advancement \n    and ethical considerations, ensuring that the benefits of AI are distributed \n    equitably across society.\"\"\"\ngrade_essay(student_essay)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T15:44:49.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}